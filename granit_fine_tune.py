# -*- coding: utf-8 -*-
"""granit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z3OzkXHuOChG0s0Fnm4CST36h-sDnG2Y
"""

#!pip install -q git+https://github.com/huggingface/transformers.git
#!pip install  -U -q trl datasets bitsandbytes peft accelerate
#!pip install cairosvg

# Import necessary libraries
from requests.exceptions import HTTPError
import requests
from PIL import Image
from io import BytesIO
import torch
import gc
import time
from datasets import load_dataset
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from trl import SFTConfig, SFTTrainer
import pandas as pd
from datasets import Dataset
import cairosvg
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction


# Configure quantization settings for memory and performance optimization
USE_QLORA = True # Less memory usage
USE_LORA = True # Update of weight matrix
#ApiQ (Activation-Preserving Initialization of Quantized LLM) : Qlora dan daha effective olabilir 2 bit
if USE_QLORA:
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True, # 4 bit use instead of 32 
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        llm_int8_skip_modules=["vision_tower", "lm_head"], #if use 4 bit for image  we can loss feature info, #lm_head  is the last layer of the model and is responsible for text generation.
        llm_int8_enable_fp32_cpu_offload=True
    )
else:
    bnb_config = None

# Define a system message for the conversation
system_message = (
    "A chat between a curious user and an artificial intelligence assistant. "
    "The assistant gives helpful, detailed, and polite answers to the user's questions."
)


def load_image(url):
    """
    Load and process images from URLs, with special handling for SVG files
    
    Args:
        url : URL of the image to load
    
    Returns:
        PIL.Image: Processed image or None if loading fails
    """
    try:
        # Set user agent to mimic browser request
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/91.0.4472.124 Safari/537.36"
            )
        }

        # Send HTTP request with timeout
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  

        content_type = response.headers.get("Content-Type", "").lower()

        # Handle SVG images by converting to PNG
        if url.lower().endswith('.svg') or "image/svg" in content_type:
            try:
                png_data = cairosvg.svg2png(bytestring=response.content)
                img = Image.open(BytesIO(png_data)).convert("RGB")
            except Exception as svg_err:
                print(f"SVG conversion error for {url}: {svg_err}")
                return None
        else:
            # Process standard image formats
            try:
                img = Image.open(BytesIO(response.content)).convert("RGB")
            except Exception as img_err:
                print(f"Image processing error for {url}: {img_err}")
                return None

        # Resize image to standard dimension
        img = img.resize((384, 384))
        return img

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error loading image from {url}: {http_err}")
        return None
  

# Load and preprocess Turkish language dataset(Since I could not use all the data because of resource package, I saved a small part of it as tsv.)
file_path = "/content/drive/MyDrive/wit_data/wit_v1.train.all-00000-of-00010_tr.tsv"
df = pd.read_csv(file_path, sep="\t", on_bad_lines="skip", encoding="utf-8")
df_filtered = df[df["language"] == "tr"]
df_filtered = df_filtered.head(200)
dataset_filtered = Dataset.from_pandas(df_filtered)



def format_data(sample):
    """
    Transform a dataset sample into a conversational structure
    
    Args:
        sample (dict): Single dataset sample
    
    Returns:
        list: Formatted conversation or None if processing fails
    """
    image = load_image(sample["image_url"])
    if image is None or sample["caption_reference_description"] is None:
        return None
    conversation = [
        # Convert to chat format (system, user, assistant)
        # This structure is also used in SFTTrainer and Granite Vision.
        {
            "role": "system",
            "content": [
                {"type": "text", "text": system_message}
            ],
        },
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "Describe this image:"}
            ],
        },
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": sample["caption_reference_description"]}
            ],
        },
    ]
    return conversation




def process_streaming_dataset(dataset_iter):
    """
    Split dataset into training and testing sets
    
    Args:
        dataset_iter (Dataset): Iterable dataset
    
    Returns:
        tuple: Training and testing datasets
    """
    data = []
    for sample in dataset_iter:
        try:
            formatted_data = format_data(sample)
            if formatted_data is not None:
                data.append(formatted_data)
        except HTTPError as e:
            # Skip the item if an HTTP error (404) occurs
            print(f"Skipping image due to error: {e}")
            continue
        except Exception as e:
            # Catch any other unexpected errors
            print(f"Skipping image due to unexpected error: {e}")
            continue

    split = int(len(data) * 0.5)
    return data[:split], data[split:]

train_dataset, test_dataset = process_streaming_dataset(dataset_filtered)

# Define model and processor IDs
model_id = "ibm-granite/granite-vision-3.2-2b"


# Load model and processor with optional quantization config
model = AutoModelForVision2Seq.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16, 
    # 4-bit weights are converted to bfloat16 before calculation and operations are performed in this format.
    # 4-bit is for storage only; Calculations are not done in 4-bit because with low bit gradients and activations are distorted and the model cannot learn or make accurate predictions.
    quantization_config=bnb_config,
)
processor = AutoProcessor.from_pretrained(model_id)
#A helper tool that matches inputs to the model.
#The processor prepares the input: resizes the images, converts the text into tokens

# Adjust image processing parameters if needed
processor.image_processor.size = {"height": 384, "width": 384}
processor.image_processor.patch_size = 16
processor.image_processor.do_resize = True
processor.image_processor.do_normalize = True

# Optionally apply LoRA for parameter-efficient fine-tuning
if USE_LORA:
    from peft import LoraConfig, get_peft_model
    #Parameter-Efficient Fine-Tuning
    #I applied PEFT with LoRA;
    # We determine to which layer will be train
    peft_config = LoraConfig(
        r=8, # Rank (low rank adaptation, few parameters are updated)
        lora_alpha=8,
        lora_dropout=0.1,
        target_modules=[name for name, _ in model.named_modules() if "language_model" in name and "_proj" in name],
        use_dora=True,
        init_lora_weights="gaussian"
    )
    model.add_adapter(peft_config) # Adding lora 
    model.enable_adapters()
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
else:
    peft_config = None

# Collate function for training
def collate_fn(examples):
    """
    Prepare batch of training examples with proper tokenization and labeling
    
    Args:
        examples (list): Training examples
    
    Returns:
        dict: Processed batch with inputs, labels, etc.
    """
    # Convert conversation into text using the processorâ€™s chat template.
    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]
    # Extract images from the user message (assumed to be at index 1)
    image_inputs = []
    for example in examples:
        image = example[1]["content"][0]["image"]
        if image.mode != "RGB":
            image = image.convert("RGB")
        image_inputs.append([image])
    batch = processor(text=texts, images=image_inputs, return_tensors="pt", padding=True)

    # Create labels from input_ids and mask out tokens until the assistant response.
    labels = batch["input_ids"].clone()
    assistant_tokens = processor.tokenizer("<|assistant|>", return_tensors="pt")["input_ids"][0]
    #I want the model to correctly predict only the text generated by the assistant.
    eos_token = processor.tokenizer("<|end_of_text|>", return_tensors="pt")["input_ids"][0]
    #Determining the point where the assistant answer ends.
    for i in range(batch["input_ids"].shape[0]):
        apply_loss = False
        for j in range(batch["input_ids"].shape[1]):
            if not apply_loss:
                labels[i][j] = -100 # These tokens are not included in the loss calculation.system and user sections are not included in the loss
            if ((j >= len(assistant_tokens) + 1) and
                torch.all(batch["input_ids"][i][j + 1 - len(assistant_tokens): j + 1] == assistant_tokens)):
                apply_loss = True
            if batch["input_ids"][i][j] == eos_token:
                apply_loss = False
    batch["labels"] = labels
    return batch



# Configure training arguments via SFTConfig
training_args = SFTConfig(
    output_dir="./content/drive/MyDrive/wit_data/google_wit_tr",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    warmup_steps=10,
    learning_rate=1e-4,
    weight_decay=0.01,  #Avoid overlearning
    logging_steps=10,
    save_strategy="steps",
    save_steps=20,
    save_total_limit=1,
    optim="adamw_torch_fused",
    bf16=True,
    push_to_hub=False,
    report_to="none",
    remove_unused_columns=False,
    gradient_checkpointing=True,
    dataset_text_field="",
    dataset_kwargs={"skip_prepare_dataset": True},
)

def clear_memory():
    """
    Clear GPU memory and collect garbage to optimize training
    """
    gc.collect()
    time.sleep(2)
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    time.sleep(2)
    gc.collect()
    time.sleep(2)
    print(f"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

clear_memory()

# Initialize SFTTrainer
#Supervised Fine-Tuning Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args, #Training arguman, parameters
    train_dataset=train_dataset, #format + test/train split
    data_collator=collate_fn,
    peft_config=peft_config,
    #Ã¼tokenizer=processor.tokenizer,
)

# Start training
trainer.train()
save_path = "./content/drive/MyDrive/wit_data"
# Save the model and processor
model.save_pretrained(save_path)
processor.save_pretrained(save_path)
